---
---

@string{aps = {American Physical Society,}}

@book{einstein1956investigations,
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,}
}

@article{einstein1950meaning,
  abbr={AJP},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  selected={false}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}


@article{jorke2025bloom,
  abbr={CHI},
  img={bloom},
  title={Bloom: Designing for LLM-Augmented Behavior Change Interactions},
  author={J{\"o}rke, Matthew and Gen{\c{c}}*, Defne and Teutschbein*, Valentin and Sapkota, Shardul and Chung, Sarah and Schmiedmayer, Paul and Campero, Maria Ines and King, Abby C and Brunskill, Emma and Landay, James A},
  journal={arXiv preprint arXiv:2510.05449},
  abstract={Large language models (LLMs) offer novel opportunities to support health behavior change, yet existing work has narrowly focused on text-only interactions. Building on decades of HCI research demonstrating the effectiveness of UI-based interactions, we present Bloom, an application for physical activity promotion that integrates an LLM-based health coaching chatbot with established UI-based interactions. As part of Bloom's development, we conducted a redteaming evaluation and contribute a safety benchmark dataset. In a four-week randomized field study (N=54) comparing Bloom to a non-LLM control, we observed important shifts in psychological outcomes: participants in the LLM condition reported stronger beliefs that activity was beneficial, greater enjoyment, and more self-compassion. Both conditions significantly increased physical activity levels, doubling the proportion of participants meeting recommended weekly guidelines, though we observed no significant differences between conditions. Instead, our findings suggest that LLMs may be more effective at shifting mindsets that precede longer-term behavior change.},
  year={2025},
  url={https://arxiv.org/abs/2510.05449},  
  html={https://arxiv.org/abs/2510.05449},
  equal={*Equal contribution},
  pdf={bloom.pdf},
  selected={true}
}

@article{shaikh2025creating,
  abbr={UIST},
  img={gum},
  award={best paper honorable mention},
  title={Creating general user models from computer use},
  author={Shaikh, Omar and Sapkota, Shardul and Rizvi, Shan and Horvitz, Eric and Park, Joon Sung and Yang, Diyi and Bernstein, Michael S},
  journal={Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology, 2025},
  abstract={Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.},
  year={2025},
  url={https://arxiv.org/abs/2505.10831},  
  html={https://arxiv.org/abs/2505.10831},
  pdf={gum.pdf},
  project={https://generalusermodels.github.io/},
  selected={true}
}

@article{jorke2024supporting,
  abbr={CHI},
  img={gptcoach},
  title={GPTCoach: Towards LLM-Based Physical Activity Coaching},
  author={J{\"o}rke, Matthew and Sapkota, Shardul and Warkenthien, Lyndsea and Vainio, Niklas and Schmiedmayer, Paul and Brunskill, Emma and Landay, James},
  journal={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  abstract={Mobile health applications show promise for scalable physical activity promotion but are often insufficiently personalized. In contrast, health coaching offers highly personalized support but can be prohibitively expensive and inaccessible. This study draws inspiration from health coaching to explore how large language models (LLMs) might address personalization challenges in mobile health. We conduct formative interviews with 12 health professionals and 10 potential coaching recipients to develop design principles for an LLM-based health coach. We then built GPTCoach, a chatbot that implements the onboarding conversation from an evidence-based coaching program, uses conversational strategies from motivational interviewing, and incorporates wearable data to create personalized physical activity plans. In a lab study with 16 participants using three months of historical data, we find promising evidence that GPTCoach gathers rich qualitative information to offer personalized support, with users feeling comfortable sharing concerns. We conclude with implications for future research on LLM-based physical activity support.},
  year={2024},
  url={https://arxiv.org/abs/2405.06061},  
  html={https://arxiv.org/abs/2405.06061},
  pdf={gptcoach.pdf},
  selected={true}
}

@article{uhlrich2024opencapmono,
  abbr={ASB},
  img={opencapmono},
  title={OpenCap Monocular: Human Movement Dynamics from a Single Smartphone Video},
  author={Uhlrich, Scott D. and Sapkota, Shardul and Falisse, Antoine and Delp, Scott L.},
  journal={American Society of Biomechanics Oral Abstract},
  abstract={The ability to easily measure the kinematics and kinetics of human movement could improve the prevention and treatment of neurological and musculoskeletal diseases. The time and expertise required to measure movement with marker-based motion capture has limited its adoption in the clinic and in large-scale research studies. We recently developed OpenCap, open-source software that estimates kinematics and kinetics using two smartphone videos [1]. OpenCap and other markerless motion capture technologies [2] lower the time and cost barriers to movement analysis by orders of magnitude [1]; however, they require multiple cameras. If we can achieve similar accuracy from a single video, the >4 billion smartphone owners around the world would have access to 3D motion capture. The computer vision field has made advances in estimating the global pose of a human mesh from a single video [3], but these algorithms are not designed for or evaluated on their biomedical utility. This study aims to develop and evaluate a pipeline for estimating kinematics and kinetics of common human movements (walking and squatting) from a single video.},
  year={2024},
  publisher={American Society of Biomechanics},
  url={https://asbweb.org/wp-content/uploads/ASB2024OralThematicAbstractBook.pdf#page=93.00}, 
  html={https://asbweb.org/wp-content/uploads/ASB2024OralThematicAbstractBook.pdf#page=93.00},
  pdf={opencapmono.pdf},
  selected={true}
}

@article{werling2024addbiomechanics,
  abbr={ECCV},
  img={addbiomechanics},
  title={AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale},
  author={Werling, Keenon and Kaneda, Janelle and Tan, Alan and Agarwal, Rishi and Skov, Six and Van Wouwe, Tom and Uhlrich, Scott and Bianco, Nicholas and Ong, Carmichael and Falisse, Antoine and Sapkota, Shardul and Chandra, Aidan and Carter, Joshua and Preatoni, Ezio and Fregly, Benjamin and Hicks, Jennifer and Delp, Scott L. and Liu, C. Karen},
  journal={European Conference on Computer Vision},
  abstract={While reconstructing human poses in 3D from inexpensive sensors has advanced significantly in recent years, quantifying the dynamics of human motion, including the muscle-generated joint torques and external forces, remains a challenge. Prior attempts to estimate physics from reconstructed human poses have been hampered by a lack of datasets with high-quality pose and force data for a variety of movements. We present the AddBiomechanics Dataset 1.0, which includes physically accurate human dynamics of 273 human subjects, over 70 hours of motion and force plate data, totaling more than 24 million frames. To construct this dataset, novel analytical methods were required, which are also reported here. We propose a benchmark for estimating human dynamics from motion using this dataset, and present several baseline results. The AddBiomechanics Dataset is publicly available at addbiomechanics.org/download data.html.},
  year={2024},
  publisher={Springer Nature Switzerland},
  url={https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12349.pdf},
  doi={https://link.springer.com/chapter/10.1007/978-3-031-73223-2_27},
  html={https://link.springer.com/chapter/10.1007/978-3-031-73223-2_27},
  pdf={addbiomechanics.pdf},
  selected={true}
}

@article{janaka2023can,
  abbr={CHI},
  img={nuwan_icon},
  title={Can Icons Outperform Text? Understanding the Role of Pictograms in OHMD Notifications},
  author={Janaka, Nuwan Nanayakkarawasam Peru Kandage and Zhao, Shengdong and Sapkota, Shardul},
  journal={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  abstract={Optical see-through head-mounted displays (OHMDs) can provide just-in-time digital assistance to users while they are engaged in ongoing tasks. However, given users’ limited attentional resources when multitasking, there is a need to concisely and accurately present information in OHMDs. Existing approaches for digital information presentation involve using either text or pictograms. While pictograms have enabled rapid recognition and easier use in warning messages and trafc signs, most studies using pictograms for digital notifcations have exhibited unfavorable results. We thus conducted a series of four iterative studies to understand how we can support efective notifcation presentation on OHMDs during multitasking scenarios. We fnd that while icon-augmented notifcations can outperform text-only notifcations, their efectiveness depends on icon familiarity, encoding density, and environmental brightness. We reveal design implications when using iconaugmented notifcations in OHMDs and present plausible reasons for the observed disparity in literature},
  year={2023},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/pdf/10.1145/3544548.3580891},
  doi={https://dl.acm.org/doi/pdf/10.1145/3544548.3580891},
  html={https://dl.acm.org/doi/pdf/10.1145/3544548.3580891},
  pdf={icons.pdf},
  selected={true}
}

@article{sapkota2021ubiquitous,
  abbr={MobileHCI},
  img={ubiquitous},
  title={Ubiquitous Interactions for Heads-Up Computing: Understanding Users’ Preferences for Subtle Interaction Techniques in Everyday Settings},
  author={Sapkota*, Shardul and Ram*, Ashwin and Zhao, Shengdong},
  journal={23rd International Conference on Mobile Human-Computer Interaction (MobileHCI'21)},
  abstract={In order to satisfy users’ information needs while incurring minimum interference to their ongoing activities, previous studies have proposed using Optical Head-mounted Displays (OHMDs) with different input techniques. However, it is unclear how these techniques compare against one another in terms of being comfortable and non-intrusive to a user’s everyday tasks. Through a wizard-of-oz study, we thus compared four subtle interaction techniques (feet, arms, thumb-index-fingers, and teeth) in three daily hands-busy tasks under different settings (giving a presentation–sitting, carrying bags–walking, and folding clothes–standing). We found that while each interaction technique has its niche, thumb-index-finger interaction has the best overall balance and is most preferred as a cross-scenario subtle interaction technique for smart glasses. We provide further evaluation of thumb-index-finger interaction with an in-the-wild study with 8 users. Our results contribute to an enhanced understanding of user preferences for subtle interaction techniques with smart glasses for everyday use.},
  year={2021},
  publisher={ACM New York, NY, USA},  
  equal={*Equal contribution},
  pdf={ubiquitous.pdf},
  selected={true}
}

@article{kaluarachchi2021eyeknowyou,
  abbr={MobileHCI},
  img={eyeknowyou},
  title={EyeKnowYou: A DIY Toolkit to Support Monitoring Cognitive Load and Actual Screen Time using a Head-Mounted Webcam},
  author={Kaluarachchi, Tharindu and Sapkota, Shardul and Taradel, Jules and Thevenon, Aristée and Matthies, Denys J.C. and Nanayakkara, Suranga},
  journal={Extended Abstracts of the 23rd International Conference on Mobile Human-Computer Interaction (MobileHCI'21)},
  abstract={Studies show that frequent screen exposure and increased cognitive load can cause mental-health issues. Although expensive systems capable of detecting cognitive load and timers counting on-screen time exist, literature has yet to demonstrate measuring both fac- tors across devices. To address this, we propose an inexpensive DIY-approach using a single head-mounted webcam capturing the user’s eye. By classifying camera feed using a 3D Convolutional Neural Network, we can determine increased cognitive load and actual screen time. This works because the camera feed contains corneal surface reflection, as well as physiological parameters that contain information on cognitive load. Even with a small data set, we were able to develop generalised models showing 70% accuracy. To increase the models’ accuracy, we seek the community’s help by contributing more raw data. Therefore, we provide an opensource software and a DIY-guide to make our toolkit accessible to human factors researchers without an engineering background.},
  year={2021},
  publisher={ACM New York, NY, USA},  
  pdf={eyeknowyou.pdf},
  selected={true}
}

@article{zhang2021moment,
  abbr={Sensors},
  img={gradCPT},
  title={Moment-to-Moment Continuous Attention Fluctuation Monitoring through Consumer-Grade EEG Device},
  author={Zhang*, Shan and Yan*, Zihan and Sapkota, Shardul and Zhao, Shengdong and Ooi, Wei Tsang},
  journal={Sensors},
  abstract={While numerous studies have explored using various sensing techniques to measure attention states, moment-to-moment attention fluctuation measurement is unavailable. To bridge this gap, we applied a novel paradigm in psychology, the gradual-onset continuous performance task (gradCPT), to collect the ground truth of attention states. GradCPT allows for the precise labeling of attention fluctuation on an 800 ms time scale. We then developed a new technique for measuring continuous attention fluctuation, based on a machine learning approach that uses the spectral properties of EEG signals as the main features. We demonstrated that, even using a consumer grade EEG device, the detection accuracy of moment-to-moment attention fluctuations was 73.49%. Next, we empirically validated our technique in a video learning scenario and found that our technique match with the classification obtained through thought probes, with an average F1 score of 0.77. Our results suggest the effectiveness of using gradCPT as a ground truth labeling method and the feasibility of using consumer-grade EEG devices for continuous attention fluctuation detection.},
  equal={*Equal contribution},
  volume={21},
  number={10},
  pages={3419},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8156270/},
  doi={10.3390/s21103419},
  html={https://www.mdpi.com/1424-8220/21/10/3419},
  pdf={gradCPT.pdf},
  selected={true}
}

@article{chan2020prompto,
  abbr={IMWUT},
  img={prompto},
  title={Prompto: Investigating Receptivity to Prompts Based on Cognitive Load from Memory Training Conversational Agent},
  author={Chan, Samantha WT and Sapkota, Shardul and Mathews, Rebecca and Zhang, Haimo and Nanayakkara, Suranga},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  abstract={Prospective memory lapses, which involve forgetting to perform intended actions, affect independent living in older adults. Although memory training using smartphone applications could address them, users are sometimes unaware of available times for training or forget about it, presenting a need for proactive prompts. Existing applications mostly provide time-based prompts and prompts based on users' cognitive contexts remain an under-explored area. We developed Prompto, a conversational memory coach that detects physiological signals to suggest training sessions when users are relaxed and potentially more receptive. Our study with 21 older adults showed that users were more receptive to prompts and memory training under low cognitive load than under high cognitive load. Interviews and an in-the-wild deployment of Prompto indicated that majority of users appreciated the concept, found it helpful and were likely to respond to its prompts. We contribute towards developing technologies with cognitive context-aware prompting based on users' physiological readings.},
  volume={4},
  number={4},
  pages={1--23},
  year={2020},
  publisher={ACM New York, NY, USA},
  url={https://dl.acm.org/doi/10.1145/3432190},
  doi={https://dl.acm.org/doi/10.1145/3432190},
  html={https://dl.acm.org/doi/10.1145/3432190},
  pdf={prompto.pdf},
  selected={true}
}

@article{vega2019byte,
  abbr={CHI EA},
  img={byteit},
  author          = {Vega G{\'a}lvez, Tom{\'a}s and Sapkota, Shardul and Dancu, Alexandru and Maes, Pattie},
  title           = {Byte. it: Discreet Teeth Gestures for Mobile Device Interaction},
  journal         = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
  abstract={Byte.it is an exploration of the feasibility of using miniaturized, discreet hardware for teeth-clicking as hands-free input for wearable computing. Prior work has been able to identify teeth-clicking of different teeth groups. Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teeth-clicks for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid and mandibular condyle can be used to classify teeth-clicking of four different teeth groups with an accuracy of 89%.},
  pages           = {1--6},
  pdf             = {},
  year            = {2019},
  url={https://dl.acm.org/doi/10.1145/3290607.3312925},
  doi={https://dl.acm.org/doi/10.1145/3290607.3312925},
  html={https://dl.acm.org/doi/10.1145/3290607.3312925},
  pdf={byteit.pdf},
  selected={true}
}