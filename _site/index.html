<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Shardul  Sapkota</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>S</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          
          <!-- CV -->
          <li class="nav-item">
            <a class="nav-link" href="/assets/pdf/Shardul_CV.pdf" target="_blank">
              cv              
                <span class="sr-only">(current)</span>              
            </a>
          </li>                    
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Shardul</span>  Sapkota
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
        <div class="address">
          
        </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am a Machine Learning Engineer at <a href="https://shopee.sg/" target="_blank">Shopee</a>, where I work with the team responsible for training neural networks for its recommender system. Before joining Shopee, I was fortunate to have been able to study and conduct research on Human-Computer Interaction at the <a href="https://www.nus-hci.org/v2/" target="_blank">NUS-HCI Lab</a>, the <a href="http://www.ahlab.org/" target="_blank">Augmented Human Lab</a> (Auckland Bioengineering Institute), and the <a href="https://www.media.mit.edu/groups/fluid-interfaces/" target="_blank">Fluid Interfaces Group</a> (MIT Media Lab). I received my undergraduate degree from <a href="https://www.yale-nus.edu.sg/" target="_blank">Yale-NUS College</a>, where my thesis focused on using physiological signals to detect changes in people’s attentional states in real-time.</p>

<p>I am passionate about building interfaces and sensing technologies to enhance people’s sensory and cognitive abilities. The result is often a wearable device that has been programmed for real-life interventions.</p>

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com){:target="\_blank"}. You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/){:target="\_blank"} and [Academicons](https://jpswalsh.github.io/academicons/){:target="\_blank"}, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

    </div>

    <!-- <hr>

     -->

    <hr>

    
      <div class="publications">
  <h2>publications</h2>
  <ol class="bibliography"><li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/ubiquitous.jpg">
    <!--  -->
  
  </div>

  <div id="sapkota2021ubiquitous" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Ubiquitous Interactions for Heads-Up Computing: Understanding Users’ Preferences for Subtle Interaction Techniques in Everyday Settings</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota*,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Ashwin Ram*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shengdong Zhao
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Denotes equal contribution</em>
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/ubiquitous.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In order to satisfy users’ information needs while incurring minimum interference to their ongoing activities, previous studies have proposed using Optical Head-mounted Displays (OHMDs) with different input techniques. However, it is unclear how these techniques compare against one another in terms of being comfortable and non-intrusive to a user’s everyday tasks. Through a wizard-of-oz study, we thus compared four subtle interaction techniques (feet, arms, thumb-index-fingers, and teeth) in three daily hands-busy tasks under different settings (giving a presentation–sitting, carrying bags–walking, and folding clothes–standing). We found that while each interaction technique has its niche, thumb-index-finger interaction has the best overall balance and is most preferred as a cross-scenario subtle interaction technique for smart glasses. We provide further evaluation of thumb-index-finger interaction with an in-the-wild study with 8 users. Our results contribute to an enhanced understanding of user preferences for subtle interaction techniques with smart glasses for everyday use.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/ubiquitous.jpg">
    <!--  -->
  
  </div>

  <div id="sapkota2021ubiquitous" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Ubiquitous Interactions for Heads-Up Computing: Understanding Users’ Preferences for Subtle Interaction Techniques in Everyday Settings</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota*,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Ashwin Ram*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shengdong Zhao
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Denotes equal contribution</em>
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/ubiquitous.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In order to satisfy users’ information needs while incurring minimum interference to their ongoing activities, previous studies have proposed using Optical Head-mounted Displays (OHMDs) with different input techniques. However, it is unclear how these techniques compare against one another in terms of being comfortable and non-intrusive to a user’s everyday tasks. Through a wizard-of-oz study, we thus compared four subtle interaction techniques (feet, arms, thumb-index-fingers, and teeth) in three daily hands-busy tasks under different settings (giving a presentation–sitting, carrying bags–walking, and folding clothes–standing). We found that while each interaction technique has its niche, thumb-index-finger interaction has the best overall balance and is most preferred as a cross-scenario subtle interaction technique for smart glasses. We provide further evaluation of thumb-index-finger interaction with an in-the-wild study with 8 users. Our results contribute to an enhanced understanding of user preferences for subtle interaction techniques with smart glasses for everyday use.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/eyeknowyou.jpg">
    <!--  -->
  
  </div>

  <div id="kaluarachchi2021eyeknowyou" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        EyeKnowYou: A DIY Toolkit to Support Monitoring Cognitive Load and Actual Screen Time using a Head-Mounted Webcam</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tharindu Kaluarachchi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Jules Taradel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Aristée Thevenon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Denys J.C. Matthies,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/eyeknowyou.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Studies show that frequent screen exposure and increased cognitive load can cause mental-health issues. Although expensive systems capable of detecting cognitive load and timers counting on-screen time exist, literature has yet to demonstrate measuring both fac- tors across devices. To address this, we propose an inexpensive DIY-approach using a single head-mounted webcam capturing the user’s eye. By classifying camera feed using a 3D Convolutional Neural Network, we can determine increased cognitive load and actual screen time. This works because the camera feed contains corneal surface reflection, as well as physiological parameters that contain information on cognitive load. Even with a small data set, we were able to develop generalised models showing 70% accuracy. To increase the models’ accuracy, we seek the community’s help by contributing more raw data. Therefore, we provide an opensource software and a DIY-guide to make our toolkit accessible to human factors researchers without an engineering background.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/eyeknowyou.jpg">
    <!--  -->
  
  </div>

  <div id="kaluarachchi2021eyeknowyou" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        EyeKnowYou: A DIY Toolkit to Support Monitoring Cognitive Load and Actual Screen Time using a Head-Mounted Webcam</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tharindu Kaluarachchi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Jules Taradel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Aristée Thevenon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Denys J.C. Matthies,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/eyeknowyou.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Studies show that frequent screen exposure and increased cognitive load can cause mental-health issues. Although expensive systems capable of detecting cognitive load and timers counting on-screen time exist, literature has yet to demonstrate measuring both fac- tors across devices. To address this, we propose an inexpensive DIY-approach using a single head-mounted webcam capturing the user’s eye. By classifying camera feed using a 3D Convolutional Neural Network, we can determine increased cognitive load and actual screen time. This works because the camera feed contains corneal surface reflection, as well as physiological parameters that contain information on cognitive load. Even with a small data set, we were able to develop generalised models showing 70% accuracy. To increase the models’ accuracy, we seek the community’s help by contributing more raw data. Therefore, we provide an opensource software and a DIY-guide to make our toolkit accessible to human factors researchers without an engineering background.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">Sensors</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gradCPT.jpg">
    <!--  -->
  
  </div>

  <div id="zhang2021moment" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Moment-to-Moment Continuous Attention Fluctuation Monitoring through Consumer-Grade EEG Device</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Shan Zhang*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Zihan Yan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shengdong Zhao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wei Tsang Ooi
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Sensors</em>
      
      
        2021
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Denotes equal contribution</em>
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://www.mdpi.com/1424-8220/21/10/3419" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gradCPT.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While numerous studies have explored using various sensing techniques to measure attention states, moment-to-moment attention fluctuation measurement is unavailable. To bridge this gap, we applied a novel paradigm in psychology, the gradual-onset continuous performance task (gradCPT), to collect the ground truth of attention states. GradCPT allows for the precise labeling of attention fluctuation on an 800 ms time scale. We then developed a new technique for measuring continuous attention fluctuation, based on a machine learning approach that uses the spectral properties of EEG signals as the main features. We demonstrated that, even using a consumer grade EEG device, the detection accuracy of moment-to-moment attention fluctuations was 73.49%. Next, we empirically validated our technique in a video learning scenario and found that our technique match with the classification obtained through thought probes, with an average F1 score of 0.77. Our results suggest the effectiveness of using gradCPT as a ground truth labeling method and the feasibility of using consumer-grade EEG devices for continuous attention fluctuation detection.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">Sensors</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gradCPT.jpg">
    <!--  -->
  
  </div>

  <div id="zhang2021moment" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Moment-to-Moment Continuous Attention Fluctuation Monitoring through Consumer-Grade EEG Device</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Shan Zhang*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Zihan Yan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shengdong Zhao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wei Tsang Ooi
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Sensors</em>
      
      
        2021
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Denotes equal contribution</em>
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://www.mdpi.com/1424-8220/21/10/3419" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gradCPT.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While numerous studies have explored using various sensing techniques to measure attention states, moment-to-moment attention fluctuation measurement is unavailable. To bridge this gap, we applied a novel paradigm in psychology, the gradual-onset continuous performance task (gradCPT), to collect the ground truth of attention states. GradCPT allows for the precise labeling of attention fluctuation on an 800 ms time scale. We then developed a new technique for measuring continuous attention fluctuation, based on a machine learning approach that uses the spectral properties of EEG signals as the main features. We demonstrated that, even using a consumer grade EEG device, the detection accuracy of moment-to-moment attention fluctuations was 73.49%. Next, we empirically validated our technique in a video learning scenario and found that our technique match with the classification obtained through thought probes, with an average F1 score of 0.77. Our results suggest the effectiveness of using gradCPT as a ground truth labeling method and the feasibility of using consumer-grade EEG devices for continuous attention fluctuation detection.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">IMWUT</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/prompto.jpg">
    <!--  -->
  
  </div>

  <div id="chan2020prompto" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Prompto: Investigating Receptivity to Prompts Based on Cognitive Load from Memory Training Conversational Agent</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Samantha WT Chan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Rebecca Mathews,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Haimo Zhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>
      
      
        2020
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3432190" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/prompto.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Prospective memory lapses, which involve forgetting to perform intended actions, affect independent living in older adults. Although memory training using smartphone applications could address them, users are sometimes unaware of available times for training or forget about it, presenting a need for proactive prompts. Existing applications mostly provide time-based prompts and prompts based on users’ cognitive contexts remain an under-explored area. We developed Prompto, a conversational memory coach that detects physiological signals to suggest training sessions when users are relaxed and potentially more receptive. Our study with 21 older adults showed that users were more receptive to prompts and memory training under low cognitive load than under high cognitive load. Interviews and an in-the-wild deployment of Prompto indicated that majority of users appreciated the concept, found it helpful and were likely to respond to its prompts. We contribute towards developing technologies with cognitive context-aware prompting based on users’ physiological readings.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">IMWUT</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/prompto.jpg">
    <!--  -->
  
  </div>

  <div id="chan2020prompto" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Prompto: Investigating Receptivity to Prompts Based on Cognitive Load from Memory Training Conversational Agent</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Samantha WT Chan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Rebecca Mathews,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Haimo Zhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>
      
      
        2020
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3432190" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/prompto.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Prospective memory lapses, which involve forgetting to perform intended actions, affect independent living in older adults. Although memory training using smartphone applications could address them, users are sometimes unaware of available times for training or forget about it, presenting a need for proactive prompts. Existing applications mostly provide time-based prompts and prompts based on users’ cognitive contexts remain an under-explored area. We developed Prompto, a conversational memory coach that detects physiological signals to suggest training sessions when users are relaxed and potentially more receptive. Our study with 21 older adults showed that users were more receptive to prompts and memory training under low cognitive load than under high cognitive load. Interviews and an in-the-wild deployment of Prompto indicated that majority of users appreciated the concept, found it helpful and were likely to respond to its prompts. We contribute towards developing technologies with cognitive context-aware prompting based on users’ physiological readings.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI EA</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/byteit.jpg">
    <!--  -->
  
  </div>

  <div id="vega2019byte" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Byte. it: discreet teeth gestures for mobile device interaction</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tomás Vega Gálvez,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Alexandru Dancu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pattie Maes
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2019
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3290607.3312925" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/byteit.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Byte.it is an exploration of the feasibility of using miniaturized, discreet hardware for teeth-clicking as hands-free input for wearable computing. Prior work has been able to identify teeth-clicking of different teeth groups. Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teeth-clicks for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid and mandibular condyle can be used to classify teeth-clicking of four different teeth groups with an accuracy of 89%.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI EA</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/byteit.jpg">
    <!--  -->
  
  </div>

  <div id="vega2019byte" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Byte. it: discreet teeth gestures for mobile device interaction</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tomás Vega Gálvez,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Alexandru Dancu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pattie Maes
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2019
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3290607.3312925" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/byteit.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Byte.it is an exploration of the feasibility of using miniaturized, discreet hardware for teeth-clicking as hands-free input for wearable computing. Prior work has been able to identify teeth-clicking of different teeth groups. Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teeth-clicks for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid and mandibular condyle can be used to classify teeth-clicking of four different teeth groups with an accuracy of 89%.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    <!-- <hr class="custom"> -->
    <hr>

    
    <!-- <div class="social">
      <div class="contact-icons">
        

    <a href="https://scholar.google.com/citations?user=VLqcMi8AAAAJ&hl" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


    <a href="https://github.com/sapkotashardul" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
    <a href="https://www.linkedin.com/in/sapkotashardul" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>










      </div>
      <div class="contact-note"></div>
    </div> -->
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2021 Shardul  Sapkota.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
