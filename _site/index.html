<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Shardul  Sapkota</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>S</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          
          <!-- CV -->
          <li class="nav-item">
            <a class="nav-link" href="/assets/pdf/Shardul_CV.pdf" target="_blank">
              curriculum vitae              
                <span class="sr-only">(current)</span>              
            </a>
          </li>                    
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Shardul</span>  Sapkota
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
        <div class="address">
          Photo Credit: Angad Srivastava
        </div>
        <div class="contact-icons" style="
        display: flex;           /* Enable flexbox layout */
        justify-content: center; /* Center align horizontally */
        gap: 15px;">
        <a href="https://scholar.google.com/citations?user=VLqcMi8AAAAJ&amp;hl" target="_blank" title="Google Scholar"  style="font-size: 30px;"><i class="ai ai-google-scholar"></i></a>
        <a href="https://github.com/sapkotashardul" target="_blank" title="GitHub"  style="font-size: 30px;"><i class="fab fa-github"></i></a>
        <a href="https://www.linkedin.com/in/sapkotashardul" target="_blank" title="LinkedIn" style="font-size: 30px;"><i class="fab fa-linkedin" style="font-size: 30px;"></i></a>
        </div>              
      
    </div>
    

    <div class="clearfix">
      <p>I am a third year Computer Science PhD student at Stanford University, where I am advised by Professors <a href="https://www.landay.org/" target="_blank">James Landay</a> and <a href="https://nmbl.stanford.edu/people/scott-delp/" target="_blank">Scott Delp</a>.</p>

<p>My research focuses on designing sensing systems and interfaces for health and wellbeing. I am currently working on developing new approaches to unobtrusively monitor health by understanding how people interact with their everyday environment (e.g., using computer vision to perform fall risk assessment).</p>

<!-- I am passionate about building large-scale and accessible tools that detect and respond to subtle changes in cognitive state and social context. These technologies often enable novel applications in health, learning, and productivity. -->

<p>Before Stanford, I was a Senior Machine Learning Engineer at <a href="https://shopee.sg/" target="_blank">Shopee</a>, where I worked with the team responsible for training neural networks for its advertisement and recommender systems. Prior to joining Shopee, I was fortunate to have been able to study and conduct research on Human-Computer Interaction at the <a href="https://www.nus-hci.org/" target="_blank">NUS-HCI Lab</a> with Professor <a href="https://shengdongzhao.com/" target="_blank">Shengdong Zhao</a>, the <a href="http://www.ahlab.org/" target="_blank">Augmented Human Lab</a> (Auckland Bioengineering Institute) with Professor <a href="https://suranga.info/" target="_blank">Suranga Nanayakkara</a>, and the <a href="https://www.media.mit.edu/groups/fluid-interfaces/" target="_blank">Fluid Interfaces Group</a> (MIT Media Lab) with <a href="https://www.linkedin.com/in/vegatomas" target="_blank">Tomás Vega</a>. I received my undergraduate degree from <a href="https://www.yale-nus.edu.sg/" target="_blank">Yale-NUS College</a>, where my thesis focused on using physiological signals to detect changes in people’s attentional states in real-time.</p>

<!-- I am passionate about building interfaces and sensing technologies to enhance people's sensory and cognitive abilities. The result is often a wearable device that has been programmed for real-life interventions. 
 -->
<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com){:target="\_blank"}. You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/){:target="\_blank"} and [Academicons](https://jpswalsh.github.io/academicons/){:target="\_blank"}, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

    </div>

    <!-- <hr>

     -->

    <hr>

    
      <div class="publications">
  <h2>publications</h2>
  <ol class="bibliography"><li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/bloom.jpg">
    <!--  -->
  
  </div>

  <div id="jorke2025bloom" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Bloom: Designing for LLM-Augmented Behavior Change Interactions</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Matthew Jörke,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Defne Genç,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Valentin Teutschbein,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Sarah Chung,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Paul Schmiedmayer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Maria Ines Campero,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Abby C King,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Emma Brunskill,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and James A Landay
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>arXiv preprint arXiv:2510.05449</em>
      
      
        2025
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2510.05449" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/bloom.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large language models (LLMs) offer novel opportunities to support health behavior change, yet existing work has narrowly focused on text-only interactions. Building on decades of HCI research demonstrating the effectiveness of UI-based interactions, we present Bloom, an application for physical activity promotion that integrates an LLM-based health coaching chatbot with established UI-based interactions. As part of Bloom’s development, we conducted a redteaming evaluation and contribute a safety benchmark dataset. In a four-week randomized field study (N=54) comparing Bloom to a non-LLM control, we observed important shifts in psychological outcomes: participants in the LLM condition reported stronger beliefs that activity was beneficial, greater enjoyment, and more self-compassion. Both conditions significantly increased physical activity levels, doubling the proportion of participants meeting recommended weekly guidelines, though we observed no significant differences between conditions. Instead, our findings suggest that LLMs may be more effective at shifting mindsets that precede longer-term behavior change.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/bloom.jpg">
    <!--  -->
  
  </div>

  <div id="jorke2025bloom" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Bloom: Designing for LLM-Augmented Behavior Change Interactions</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Matthew Jörke,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Defne Genç,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Valentin Teutschbein,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Sarah Chung,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Paul Schmiedmayer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Maria Ines Campero,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Abby C King,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Emma Brunskill,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and James A Landay
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>arXiv preprint arXiv:2510.05449</em>
      
      
        2025
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2510.05449" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/bloom.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large language models (LLMs) offer novel opportunities to support health behavior change, yet existing work has narrowly focused on text-only interactions. Building on decades of HCI research demonstrating the effectiveness of UI-based interactions, we present Bloom, an application for physical activity promotion that integrates an LLM-based health coaching chatbot with established UI-based interactions. As part of Bloom’s development, we conducted a redteaming evaluation and contribute a safety benchmark dataset. In a four-week randomized field study (N=54) comparing Bloom to a non-LLM control, we observed important shifts in psychological outcomes: participants in the LLM condition reported stronger beliefs that activity was beneficial, greater enjoyment, and more self-compassion. Both conditions significantly increased physical activity levels, doubling the proportion of participants meeting recommended weekly guidelines, though we observed no significant differences between conditions. Instead, our findings suggest that LLMs may be more effective at shifting mindsets that precede longer-term behavior change.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">UIST</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gum.jpg">
    <!--  -->
  
  </div>

  <div id="shaikh2025creating" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Creating general user models from computer use</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Omar Shaikh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shan Rizvi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Eric Horvitz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Joon Sung Park,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Diyi Yang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Michael S Bernstein
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology, 2025</em>
      
      
        2025
      
      
        <span class="award-pill">best paper honorable mention</span>
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2505.10831" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gum.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they’re attending from messages with a friend. Or recognize that a user is struggling with a collaborator’s feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user’s behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn’t think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">UIST</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gum.jpg">
    <!--  -->
  
  </div>

  <div id="shaikh2025creating" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Creating general user models from computer use</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Omar Shaikh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shan Rizvi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Eric Horvitz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Joon Sung Park,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Diyi Yang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Michael S Bernstein
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology, 2025</em>
      
      
        2025
      
      
        <span class="award-pill">best paper honorable mention</span>
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2505.10831" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gum.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they’re attending from messages with a friend. Or recognize that a user is struggling with a collaborator’s feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user’s behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn’t think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gptcoach.jpg">
    <!--  -->
  
  </div>

  <div id="jorke2024supporting" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        GPTCoach: Towards LLM-Based Physical Activity Coaching</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Matthew Jörke,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Lyndsea Warkenthien,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Niklas Vainio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Paul Schmiedmayer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Emma Brunskill,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and James Landay
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2024
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2405.06061" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gptcoach.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Mobile health applications show promise for scalable physical activity promotion but are often insufficiently personalized. In contrast, health coaching offers highly personalized support but can be prohibitively expensive and inaccessible. This study draws inspiration from health coaching to explore how large language models (LLMs) might address personalization challenges in mobile health. We conduct formative interviews with 12 health professionals and 10 potential coaching recipients to develop design principles for an LLM-based health coach. We then built GPTCoach, a chatbot that implements the onboarding conversation from an evidence-based coaching program, uses conversational strategies from motivational interviewing, and incorporates wearable data to create personalized physical activity plans. In a lab study with 16 participants using three months of historical data, we find promising evidence that GPTCoach gathers rich qualitative information to offer personalized support, with users feeling comfortable sharing concerns. We conclude with implications for future research on LLM-based physical activity support.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gptcoach.jpg">
    <!--  -->
  
  </div>

  <div id="jorke2024supporting" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        GPTCoach: Towards LLM-Based Physical Activity Coaching</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Matthew Jörke,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Lyndsea Warkenthien,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Niklas Vainio,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Paul Schmiedmayer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Emma Brunskill,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and James Landay
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2024
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://arxiv.org/abs/2405.06061" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gptcoach.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Mobile health applications show promise for scalable physical activity promotion but are often insufficiently personalized. In contrast, health coaching offers highly personalized support but can be prohibitively expensive and inaccessible. This study draws inspiration from health coaching to explore how large language models (LLMs) might address personalization challenges in mobile health. We conduct formative interviews with 12 health professionals and 10 potential coaching recipients to develop design principles for an LLM-based health coach. We then built GPTCoach, a chatbot that implements the onboarding conversation from an evidence-based coaching program, uses conversational strategies from motivational interviewing, and incorporates wearable data to create personalized physical activity plans. In a lab study with 16 participants using three months of historical data, we find promising evidence that GPTCoach gathers rich qualitative information to offer personalized support, with users feeling comfortable sharing concerns. We conclude with implications for future research on LLM-based physical activity support.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">ASB</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/opencapmono.jpg">
    <!--  -->
  
  </div>

  <div id="uhlrich2024opencapmono" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        OpenCap Monocular: Human Movement Dynamics from a Single Smartphone Video</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Scott D. Uhlrich,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Antoine Falisse,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Scott L. Delp
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>American Society of Biomechanics Oral Abstract</em>
      
      
        2024
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://asbweb.org/wp-content/uploads/ASB2024OralThematicAbstractBook.pdf#page=93.00" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/opencapmono.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The ability to easily measure the kinematics and kinetics of human movement could improve the prevention and treatment of neurological and musculoskeletal diseases. The time and expertise required to measure movement with marker-based motion capture has limited its adoption in the clinic and in large-scale research studies. We recently developed OpenCap, open-source software that estimates kinematics and kinetics using two smartphone videos [1]. OpenCap and other markerless motion capture technologies [2] lower the time and cost barriers to movement analysis by orders of magnitude [1]; however, they require multiple cameras. If we can achieve similar accuracy from a single video, the >4 billion smartphone owners around the world would have access to 3D motion capture. The computer vision field has made advances in estimating the global pose of a human mesh from a single video [3], but these algorithms are not designed for or evaluated on their biomedical utility. This study aims to develop and evaluate a pipeline for estimating kinematics and kinetics of common human movements (walking and squatting) from a single video.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">ASB</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/opencapmono.jpg">
    <!--  -->
  
  </div>

  <div id="uhlrich2024opencapmono" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        OpenCap Monocular: Human Movement Dynamics from a Single Smartphone Video</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Scott D. Uhlrich,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Antoine Falisse,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Scott L. Delp
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>American Society of Biomechanics Oral Abstract</em>
      
      
        2024
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://asbweb.org/wp-content/uploads/ASB2024OralThematicAbstractBook.pdf#page=93.00" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/opencapmono.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The ability to easily measure the kinematics and kinetics of human movement could improve the prevention and treatment of neurological and musculoskeletal diseases. The time and expertise required to measure movement with marker-based motion capture has limited its adoption in the clinic and in large-scale research studies. We recently developed OpenCap, open-source software that estimates kinematics and kinetics using two smartphone videos [1]. OpenCap and other markerless motion capture technologies [2] lower the time and cost barriers to movement analysis by orders of magnitude [1]; however, they require multiple cameras. If we can achieve similar accuracy from a single video, the >4 billion smartphone owners around the world would have access to 3D motion capture. The computer vision field has made advances in estimating the global pose of a human mesh from a single video [3], but these algorithms are not designed for or evaluated on their biomedical utility. This study aims to develop and evaluate a pipeline for estimating kinematics and kinetics of common human movements (walking and squatting) from a single video.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">ECCV</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/addbiomechanics.jpg">
    <!--  -->
  
  </div>

  <div id="werling2024addbiomechanics" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Keenon Werling,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Janelle Kaneda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Alan Tan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Rishi Agarwal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Six Skov,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Tom Van Wouwe,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Scott Uhlrich,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Nicholas Bianco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Carmichael Ong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Antoine Falisse,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Aidan Chandra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Joshua Carter,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Ezio Preatoni,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Benjamin Fregly,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Jennifer Hicks,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Scott L. Delp,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and C. Karen Liu
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>European Conference on Computer Vision</em>
      
      
        2024
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-73223-2_27" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/addbiomechanics.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While reconstructing human poses in 3D from inexpensive sensors has advanced significantly in recent years, quantifying the dynamics of human motion, including the muscle-generated joint torques and external forces, remains a challenge. Prior attempts to estimate physics from reconstructed human poses have been hampered by a lack of datasets with high-quality pose and force data for a variety of movements. We present the AddBiomechanics Dataset 1.0, which includes physically accurate human dynamics of 273 human subjects, over 70 hours of motion and force plate data, totaling more than 24 million frames. To construct this dataset, novel analytical methods were required, which are also reported here. We propose a benchmark for estimating human dynamics from motion using this dataset, and present several baseline results. The AddBiomechanics Dataset is publicly available at addbiomechanics.org/download data.html.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">ECCV</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/addbiomechanics.jpg">
    <!--  -->
  
  </div>

  <div id="werling2024addbiomechanics" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        AddBiomechanics Dataset: Capturing the Physics of Human Motion at Scale</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Keenon Werling,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Janelle Kaneda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Alan Tan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Rishi Agarwal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Six Skov,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Tom Van Wouwe,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Scott Uhlrich,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Nicholas Bianco,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Carmichael Ong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Antoine Falisse,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Aidan Chandra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Joshua Carter,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Ezio Preatoni,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Benjamin Fregly,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Jennifer Hicks,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Scott L. Delp,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and C. Karen Liu
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>European Conference on Computer Vision</em>
      
      
        2024
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-73223-2_27" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/addbiomechanics.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While reconstructing human poses in 3D from inexpensive sensors has advanced significantly in recent years, quantifying the dynamics of human motion, including the muscle-generated joint torques and external forces, remains a challenge. Prior attempts to estimate physics from reconstructed human poses have been hampered by a lack of datasets with high-quality pose and force data for a variety of movements. We present the AddBiomechanics Dataset 1.0, which includes physically accurate human dynamics of 273 human subjects, over 70 hours of motion and force plate data, totaling more than 24 million frames. To construct this dataset, novel analytical methods were required, which are also reported here. We propose a benchmark for estimating human dynamics from motion using this dataset, and present several baseline results. The AddBiomechanics Dataset is publicly available at addbiomechanics.org/download data.html.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/nuwan_icon.jpg">
    <!--  -->
  
  </div>

  <div id="janaka2023can" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Can Icons Outperform Text? Understanding the Role of Pictograms in OHMD Notifications</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Nuwan Nanayakkarawasam Peru Kandage Janaka,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shengdong Zhao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Shardul Sapkota</em>
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2023
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/pdf/10.1145/3544548.3580891" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/icons.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Optical see-through head-mounted displays (OHMDs) can provide just-in-time digital assistance to users while they are engaged in ongoing tasks. However, given users’ limited attentional resources when multitasking, there is a need to concisely and accurately present information in OHMDs. Existing approaches for digital information presentation involve using either text or pictograms. While pictograms have enabled rapid recognition and easier use in warning messages and trafc signs, most studies using pictograms for digital notifcations have exhibited unfavorable results. We thus conducted a series of four iterative studies to understand how we can support efective notifcation presentation on OHMDs during multitasking scenarios. We fnd that while icon-augmented notifcations can outperform text-only notifcations, their efectiveness depends on icon familiarity, encoding density, and environmental brightness. We reveal design implications when using iconaugmented notifcations in OHMDs and present plausible reasons for the observed disparity in literature</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/nuwan_icon.jpg">
    <!--  -->
  
  </div>

  <div id="janaka2023can" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Can Icons Outperform Text? Understanding the Role of Pictograms in OHMD Notifications</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Nuwan Nanayakkarawasam Peru Kandage Janaka,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shengdong Zhao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Shardul Sapkota,</em>
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2023
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/pdf/10.1145/3544548.3580891" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/icons.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Optical see-through head-mounted displays (OHMDs) can provide just-in-time digital assistance to users while they are engaged in ongoing tasks. However, given users’ limited attentional resources when multitasking, there is a need to concisely and accurately present information in OHMDs. Existing approaches for digital information presentation involve using either text or pictograms. While pictograms have enabled rapid recognition and easier use in warning messages and trafc signs, most studies using pictograms for digital notifcations have exhibited unfavorable results. We thus conducted a series of four iterative studies to understand how we can support efective notifcation presentation on OHMDs during multitasking scenarios. We fnd that while icon-augmented notifcations can outperform text-only notifcations, their efectiveness depends on icon familiarity, encoding density, and environmental brightness. We reveal design implications when using iconaugmented notifcations in OHMDs and present plausible reasons for the observed disparity in literature</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/ubiquitous.jpg">
    <!--  -->
  
  </div>

  <div id="sapkota2021ubiquitous" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Ubiquitous Interactions for Heads-Up Computing: Understanding Users’ Preferences for Subtle Interaction Techniques in Everyday Settings</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota*,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Ashwin Ram*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shengdong Zhao
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Equal contribution</em>
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/ubiquitous.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In order to satisfy users’ information needs while incurring minimum interference to their ongoing activities, previous studies have proposed using Optical Head-mounted Displays (OHMDs) with different input techniques. However, it is unclear how these techniques compare against one another in terms of being comfortable and non-intrusive to a user’s everyday tasks. Through a wizard-of-oz study, we thus compared four subtle interaction techniques (feet, arms, thumb-index-fingers, and teeth) in three daily hands-busy tasks under different settings (giving a presentation–sitting, carrying bags–walking, and folding clothes–standing). We found that while each interaction technique has its niche, thumb-index-finger interaction has the best overall balance and is most preferred as a cross-scenario subtle interaction technique for smart glasses. We provide further evaluation of thumb-index-finger interaction with an in-the-wild study with 8 users. Our results contribute to an enhanced understanding of user preferences for subtle interaction techniques with smart glasses for everyday use.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/ubiquitous.jpg">
    <!--  -->
  
  </div>

  <div id="sapkota2021ubiquitous" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Ubiquitous Interactions for Heads-Up Computing: Understanding Users’ Preferences for Subtle Interaction Techniques in Everyday Settings</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota*,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Ashwin Ram*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shengdong Zhao
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Equal contribution</em>
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/ubiquitous.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In order to satisfy users’ information needs while incurring minimum interference to their ongoing activities, previous studies have proposed using Optical Head-mounted Displays (OHMDs) with different input techniques. However, it is unclear how these techniques compare against one another in terms of being comfortable and non-intrusive to a user’s everyday tasks. Through a wizard-of-oz study, we thus compared four subtle interaction techniques (feet, arms, thumb-index-fingers, and teeth) in three daily hands-busy tasks under different settings (giving a presentation–sitting, carrying bags–walking, and folding clothes–standing). We found that while each interaction technique has its niche, thumb-index-finger interaction has the best overall balance and is most preferred as a cross-scenario subtle interaction technique for smart glasses. We provide further evaluation of thumb-index-finger interaction with an in-the-wild study with 8 users. Our results contribute to an enhanced understanding of user preferences for subtle interaction techniques with smart glasses for everyday use.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/eyeknowyou.jpg">
    <!--  -->
  
  </div>

  <div id="kaluarachchi2021eyeknowyou" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        EyeKnowYou: A DIY Toolkit to Support Monitoring Cognitive Load and Actual Screen Time using a Head-Mounted Webcam</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tharindu Kaluarachchi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Jules Taradel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Aristée Thevenon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Denys J.C. Matthies,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/eyeknowyou.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Studies show that frequent screen exposure and increased cognitive load can cause mental-health issues. Although expensive systems capable of detecting cognitive load and timers counting on-screen time exist, literature has yet to demonstrate measuring both fac- tors across devices. To address this, we propose an inexpensive DIY-approach using a single head-mounted webcam capturing the user’s eye. By classifying camera feed using a 3D Convolutional Neural Network, we can determine increased cognitive load and actual screen time. This works because the camera feed contains corneal surface reflection, as well as physiological parameters that contain information on cognitive load. Even with a small data set, we were able to develop generalised models showing 70% accuracy. To increase the models’ accuracy, we seek the community’s help by contributing more raw data. Therefore, we provide an opensource software and a DIY-guide to make our toolkit accessible to human factors researchers without an engineering background.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">MobileHCI</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/eyeknowyou.jpg">
    <!--  -->
  
  </div>

  <div id="kaluarachchi2021eyeknowyou" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        EyeKnowYou: A DIY Toolkit to Support Monitoring Cognitive Load and Actual Screen Time using a Head-Mounted Webcam</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tharindu Kaluarachchi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Jules Taradel,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Aristée Thevenon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Denys J.C. Matthies,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 23rd International Conference on Mobile Human-Computer Interaction (MobileHCI’21)</em>
      
      
        2021
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
      
      <a href="/assets/pdf/eyeknowyou.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Studies show that frequent screen exposure and increased cognitive load can cause mental-health issues. Although expensive systems capable of detecting cognitive load and timers counting on-screen time exist, literature has yet to demonstrate measuring both fac- tors across devices. To address this, we propose an inexpensive DIY-approach using a single head-mounted webcam capturing the user’s eye. By classifying camera feed using a 3D Convolutional Neural Network, we can determine increased cognitive load and actual screen time. This works because the camera feed contains corneal surface reflection, as well as physiological parameters that contain information on cognitive load. Even with a small data set, we were able to develop generalised models showing 70% accuracy. To increase the models’ accuracy, we seek the community’s help by contributing more raw data. Therefore, we provide an opensource software and a DIY-guide to make our toolkit accessible to human factors researchers without an engineering background.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">Sensors</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gradCPT.jpg">
    <!--  -->
  
  </div>

  <div id="zhang2021moment" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Moment-to-Moment Continuous Attention Fluctuation Monitoring through Consumer-Grade EEG Device</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Shan Zhang*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Zihan Yan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shengdong Zhao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wei Tsang Ooi
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Sensors</em>
      
      
        2021
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Equal contribution</em>
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://www.mdpi.com/1424-8220/21/10/3419" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gradCPT.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While numerous studies have explored using various sensing techniques to measure attention states, moment-to-moment attention fluctuation measurement is unavailable. To bridge this gap, we applied a novel paradigm in psychology, the gradual-onset continuous performance task (gradCPT), to collect the ground truth of attention states. GradCPT allows for the precise labeling of attention fluctuation on an 800 ms time scale. We then developed a new technique for measuring continuous attention fluctuation, based on a machine learning approach that uses the spectral properties of EEG signals as the main features. We demonstrated that, even using a consumer grade EEG device, the detection accuracy of moment-to-moment attention fluctuations was 73.49%. Next, we empirically validated our technique in a video learning scenario and found that our technique match with the classification obtained through thought probes, with an average F1 score of 0.77. Our results suggest the effectiveness of using gradCPT as a ground truth labeling method and the feasibility of using consumer-grade EEG devices for continuous attention fluctuation detection.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">Sensors</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/gradCPT.jpg">
    <!--  -->
  
  </div>

  <div id="zhang2021moment" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Moment-to-Moment Continuous Attention Fluctuation Monitoring through Consumer-Grade EEG Device</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Shan Zhang*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Zihan Yan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Shengdong Zhao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wei Tsang Ooi
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Sensors</em>
      
      
        2021
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
        <em> *Equal contribution</em>
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://www.mdpi.com/1424-8220/21/10/3419" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/gradCPT.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While numerous studies have explored using various sensing techniques to measure attention states, moment-to-moment attention fluctuation measurement is unavailable. To bridge this gap, we applied a novel paradigm in psychology, the gradual-onset continuous performance task (gradCPT), to collect the ground truth of attention states. GradCPT allows for the precise labeling of attention fluctuation on an 800 ms time scale. We then developed a new technique for measuring continuous attention fluctuation, based on a machine learning approach that uses the spectral properties of EEG signals as the main features. We demonstrated that, even using a consumer grade EEG device, the detection accuracy of moment-to-moment attention fluctuations was 73.49%. Next, we empirically validated our technique in a video learning scenario and found that our technique match with the classification obtained through thought probes, with an average F1 score of 0.77. Our results suggest the effectiveness of using gradCPT as a ground truth labeling method and the feasibility of using consumer-grade EEG devices for continuous attention fluctuation detection.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">IMWUT</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/prompto.jpg">
    <!--  -->
  
  </div>

  <div id="chan2020prompto" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Prompto: Investigating Receptivity to Prompts Based on Cognitive Load from Memory Training Conversational Agent</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Samantha WT Chan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Rebecca Mathews,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Haimo Zhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>
      
      
        2020
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3432190" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/prompto.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Prospective memory lapses, which involve forgetting to perform intended actions, affect independent living in older adults. Although memory training using smartphone applications could address them, users are sometimes unaware of available times for training or forget about it, presenting a need for proactive prompts. Existing applications mostly provide time-based prompts and prompts based on users’ cognitive contexts remain an under-explored area. We developed Prompto, a conversational memory coach that detects physiological signals to suggest training sessions when users are relaxed and potentially more receptive. Our study with 21 older adults showed that users were more receptive to prompts and memory training under low cognitive load than under high cognitive load. Interviews and an in-the-wild deployment of Prompto indicated that majority of users appreciated the concept, found it helpful and were likely to respond to its prompts. We contribute towards developing technologies with cognitive context-aware prompting based on users’ physiological readings.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">IMWUT</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/prompto.jpg">
    <!--  -->
  
  </div>

  <div id="chan2020prompto" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Prompto: Investigating Receptivity to Prompts Based on Cognitive Load from Memory Training Conversational Agent</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Samantha WT Chan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Rebecca Mathews,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                Haimo Zhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Suranga Nanayakkara
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>
      
      
        2020
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3432190" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/prompto.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Prospective memory lapses, which involve forgetting to perform intended actions, affect independent living in older adults. Although memory training using smartphone applications could address them, users are sometimes unaware of available times for training or forget about it, presenting a need for proactive prompts. Existing applications mostly provide time-based prompts and prompts based on users’ cognitive contexts remain an under-explored area. We developed Prompto, a conversational memory coach that detects physiological signals to suggest training sessions when users are relaxed and potentially more receptive. Our study with 21 older adults showed that users were more receptive to prompts and memory training under low cognitive load than under high cognitive load. Interviews and an in-the-wild deployment of Prompto indicated that majority of users appreciated the concept, found it helpful and were likely to respond to its prompts. We contribute towards developing technologies with cognitive context-aware prompting based on users’ physiological readings.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row" id="desktop_row">
  <div class="col-sm-3 abbr vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI EA</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/byteit.jpg">
    <!--  -->
  
  </div>

  <div id="vega2019byte" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Byte. it: Discreet Teeth Gestures for Mobile Device Interaction</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tomás Vega Gálvez,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Alexandru Dancu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pattie Maes
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2019
      
      
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3290607.3312925" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/byteit.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Byte.it is an exploration of the feasibility of using miniaturized, discreet hardware for teeth-clicking as hands-free input for wearable computing. Prior work has been able to identify teeth-clicking of different teeth groups. Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teeth-clicks for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid and mandibular condyle can be used to classify teeth-clicking of four different teeth groups with an accuracy of 89%.</p>
    </div>
    
  </div>
</div>

<div id="mobile_row">
  
  <div class="col-sm vcenter">
  
    <!--  -->
    <!-- <abbr class="badge">CHI EA</abbr> -->
    <img class="img-fluid custom z-depth-1 rounded" src="/assets/img/thumbnails/byteit.jpg">
    <!--  -->
  
  </div>

  <div id="vega2019byte" class="col-sm-8">
    
      <div class="title" style="margin-bottom: 6px;">
        Byte. it: Discreet Teeth Gestures for Mobile Device Interaction</div>
      <div class="author" style="margin-bottom: 6px; font-size: .85rem;">
        
          
          
          
          
          
          
            
              
                
                Tomás Vega Gálvez,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Shardul Sapkota,</em>
              
            
          
        
          
          
          
          
          
          
            
              
                
                Alexandru Dancu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Pattie Maes
                
              
            
          
        
      </div>

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
      
        <em>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</em>
      
      
        2019
      
      
      </div>
    

      <div class="periodical" style="margin-bottom: 6px; font-size: .85rem;">
        
      </div>
  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3290607.3312925" class="btn btn-sm z-depth-0" role="button" target="_blank">DOI</a>
    
    
      
      <a href="/assets/pdf/byteit.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Byte.it is an exploration of the feasibility of using miniaturized, discreet hardware for teeth-clicking as hands-free input for wearable computing. Prior work has been able to identify teeth-clicking of different teeth groups. Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teeth-clicks for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid and mandibular condyle can be used to classify teeth-clicking of four different teeth groups with an accuracy of 89%.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    <!-- <hr class="custom"> -->
    <hr>

    <!-- 
    <div class="social">
      <div class="contact-icons">
        

<a href="https://scholar.google.com/citations?user=VLqcMi8AAAAJ&hl" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/sapkotashardul" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/sapkotashardul" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>










      </div>
      <div class="contact-note"></div>
    </div>
     -->
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2025 Shardul  Sapkota.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
